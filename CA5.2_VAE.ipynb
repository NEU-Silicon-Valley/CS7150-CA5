{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a32812-a34a-467e-9107-8389315dc058",
   "metadata": {},
   "source": [
    "\n",
    "# Variational Autoencoder (VAE) for Swiss Roll Data\n",
    "\n",
    "In this notebook, you will implement a VAE for 2D Swiss Roll data. The goals are to:\n",
    "- **Understand VAE components:** the encoder, the decoder, and the reparameterization trick.\n",
    "- **Implement the reparameterization trick:** Use mean and log-variance to sample latent vectors.\n",
    "- **Train the VAE** to produce good reconstructions and generate new samples.\n",
    "  \n",
    "**Learning Objectives:**\n",
    "1. Write a correct reparameterization function:\n",
    "   - Given latent mean $ \\mu $ and log-variance $ \\log\\sigma^2 $, compute the standard deviation and sample using:  \n",
    "$$\n",
    "   z = \\mu + \\epsilon \\cdot \\sigma \\quad \\text{with} \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "2. Implement the VAE loss incorporating both reconstruction loss (MSE) and the closed-form KL divergence loss.\n",
    "$$\n",
    "\\mathrm{KL}\\big(q(z|x)\\parallel p(z)\\big) = -\\frac{1}{2} \\sum_{j=1}^{d} \\Bigl(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\Bigr),\n",
    "$$\n",
    "4. Visualize reconstructed points and compare generated samples with real Swiss Roll data.\n",
    "\n",
    "**Your Task:**  \n",
    "* Implement the **TODO** in the `reparameterize()` method.\n",
    "* Implement the **TODO** in the `compute_vae_loss()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d1f53-4f7f-45e1-aa87-0778f53654ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef78311-a0ce-40ed-9503-c9e154d9dbd8",
   "metadata": {},
   "source": [
    "\n",
    "## Swiss Roll Data Generation\n",
    "\n",
    "This function generates synthetic 2D Swiss Roll data. We'll use it to train the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59271a4-2794-40cf-8d49-d10193d47b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll data points.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of points to generate.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    # Random parameter t for the roll\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples))\n",
    "    x = t * torch.cos(t)  # Generate x coordinates\n",
    "    y = t * torch.sin(t)  # Generate y coordinates\n",
    "    data = torch.stack([x, y], dim=1) / 15.0  # Scale the data down\n",
    "    return data\n",
    "\n",
    "# Visualize generated data:\n",
    "data = generate_swiss_roll(n_samples=10000)\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)\n",
    "plt.title('Synthetic Swiss Roll Distribution')\n",
    "plt.axis('square')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cceaedc-e6c9-42db-9345-0e65990292d5",
   "metadata": {},
   "source": [
    "## VAE Model Definition\n",
    "\n",
    "The VAE comprises an encoder, a reparameterization module, and a decoder.  \n",
    "**TODO:** Implement the reparameterization trick in the `reparameterize()` function.\n",
    "\n",
    "*Hints:*\n",
    "- Compute the standard deviation using:\n",
    "  ```python\n",
    "  std = torch.exp(0.5 * logvar)\n",
    "  ```\n",
    "- Draw $ \\epsilon $ from a standard normal distribution with the same shape as std using:\n",
    "```python\n",
    "eps = torch.randn_like(std)\n",
    "```\n",
    "Then return $ z = \\mu + eps \\times std $.\n",
    "\n",
    "Fill in the code where the `TODO` is placed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e266c",
   "metadata": {},
   "source": [
    "[5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45ad96-bac4-4b1e-b7a6-c73611f83704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for 2D data.\n",
    "    The encoder maps 2D input points to latent parameters.\n",
    "    The decoder maps latent samples back to 2D points.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder network: transforms 2D input to a hidden representation.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Layers to produce the latent mean and log variance.\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder network: transforms latent vector back to 2D output.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Output: 2D coordinate reconstruction.\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode the input to latent distribution parameters.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: sample a latent vector z from the Gaussian distribution\n",
    "        defined by mu and logvar.\n",
    "        \n",
    "        **TODO:** Implement the reparameterization trick. Replace None with your code\n",
    "        \n",
    "        Expected steps:\n",
    "          1. Compute standard deviation: std = exp(0.5 * logvar)\n",
    "          2. Sample epsilon from a standard normal distribution with the same dimensions as std.\n",
    "          3. Return mu + eps * std\n",
    "        \"\"\"\n",
    "        # ----- TODO: Student Implementation Starts Here -----\n",
    "        std = None  # Compute the standard deviation from log variance\n",
    "        eps = None  # Sample epsilon ~ N(0,I) with same shape as std\n",
    "        return None # Return the reparameterized latent vector\n",
    "        # ----- TODO: Student Implementation Ends Here -----\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode a latent vector z to reconstructed points.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform a full forward pass: encode -> reparameterize -> decode.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# You may test the forward pass with some dummy data.\n",
    "# test_data = generate_swiss_roll(5)\n",
    "# output, _, _ = VAE()(test_data)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06d2fc-1efc-4a73-b96c-73e13d9486a2",
   "metadata": {},
   "source": [
    "## Training the VAE\n",
    "\n",
    "In this cell, we'll train the VAE on Swiss Roll data. The loss consists of:\n",
    "- **Reconstruction Loss:** Mean Squared Error between input and reconstruction.\n",
    "- **KL Divergence Loss:** Measures how much the encoded distribution deviates from a unit Gaussian.\n",
    "\n",
    "The total loss is:\n",
    "$$\n",
    "\\text{loss} = \\text{recon\\_loss} + \\beta \\times \\text{KL\\_loss}\n",
    "$$\n",
    "where $\\beta$ scales the KL term; which controls the trade-off:\n",
    "- Higher β → more structured latent space, potentially worse reconstructions\n",
    "- Lower β → better reconstructions, but latent space may not be smooth/continuous\n",
    "\n",
    "Typical values: 0.001 to 0.1\n",
    "\n",
    "</br>\n",
    "\n",
    "### Reconstruction Loss\n",
    "\n",
    "In this VAE implementation, the reconstruction loss is measured by the mean squared error (MSE) between the original input $x$ and its reconstruction $\\hat{x}$. In mathematical terms, this loss is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = \\frac{1}{N} \\sum_{i=1}^{N} \\|x_i - \\hat{x}_i\\|^2,\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data points in the batch. This term encourages the decoder to produce outputs that closely resemble the input data.\n",
    "\n",
    "### KL Divergence Loss\n",
    "\n",
    "The VAE also includes a regularization term based on the Kullback-Leibler (KL) divergence. The encoder produces a latent representation described by a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ (often represented via $\\log \\sigma^2$ in the code). The goal is to keep the approximate posterior $q(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$ close to the standard normal prior $p(z) = \\mathcal{N}(0, I)$.\n",
    "\n",
    "The KL divergence between these two distributions is given by:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}\\big(q(z|x)\\parallel p(z)\\big) = -\\frac{1}{2} \\sum_{j=1}^{d} \\Bigl(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\Bigr),\n",
    "$$\n",
    "\n",
    "where $d$ is the dimensionality of the latent space. In the code, this is computed with:\n",
    "\n",
    "```python\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "```\n",
    "\n",
    "Here, `logvar` represents $\\log \\sigma^2$ and `logvar.exp()` computes $\\sigma^2$.\n",
    "\n",
    "\n",
    "### Detailed Derivation of the KL Term\n",
    "\n",
    "The unusual-looking but simple KL divergence loss is a key that makes VAE training work.  It is\n",
    "the KL divergence between the marginal (prior) Gaussian distribution and the estimated (posterior)\n",
    "Gaussian distribution proposed by the encoder.  This KL divergence quantifies the exact amount of\n",
    "information that passes from the encoder to the decoder.\n",
    "\n",
    "To derive the formula, we start with the definition of the KL divergence between the approximate posterior and the prior:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(q(z|x) \\parallel p(z)) = \\int q(z|x) \\log \\frac{q(z|x)}{p(z)} \\, dz.\n",
    "$$\n",
    "\n",
    "For a diagonal Gaussian, the densities are given by\n",
    "\n",
    "$$\n",
    "q(z|x) = \\prod_{j=1}^d \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}} \\exp\\left(-\\frac{(z_j-\\mu_j)^2}{2\\sigma_j^2}\\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "p(z) = \\prod_{j=1}^d \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_j^2}{2}\\right).\n",
    "$$\n",
    "\n",
    "Because the KL divergence decomposes over the dimensions, we can focus on a single dimension and then sum the results. For one dimension, we have:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}\\big(\\mathcal{N}(\\mu, \\sigma^2) \\parallel \\mathcal{N}(0,1)\\big) = \\int \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right) \\log \\frac{\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)} \\, dz.\n",
    "$$\n",
    "\n",
    "First, simplify the fraction inside the logarithm:\n",
    "\n",
    "$$\n",
    "\\frac{q(z|x)}{p(z)} = \\frac{1}{\\sqrt{\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2}\\right).\n",
    "$$\n",
    "\n",
    "Taking the logarithm gives:\n",
    "\n",
    "$$\n",
    "\\log \\frac{q(z|x)}{p(z)} = -\\frac{1}{2}\\log \\sigma^2 -\\frac{(z-\\mu)^2}{2\\sigma^2} + \\frac{z^2}{2}.\n",
    "$$\n",
    "\n",
    "Now, taking the expectation with respect to $q(z|x)$, we have:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL} = -\\frac{1}{2}\\log \\sigma^2 - \\frac{1}{2\\sigma^2} \\mathbb{E}_{q}[(z-\\mu)^2] + \\frac{1}{2} \\mathbb{E}_{q}[z^2].\n",
    "$$\n",
    "\n",
    "For $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, it is known that $\\mathbb{E}_{q}[(z-\\mu)^2] = \\sigma^2$ and $\\mathbb{E}_{q}[z^2] = \\sigma^2 + \\mu^2$. Substituting these expectations into the expression yields:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL} = -\\frac{1}{2}\\log \\sigma^2 - \\frac{1}{2} + \\frac{1}{2} (\\sigma^2 + \\mu^2).\n",
    "$$\n",
    "\n",
    "Thus, the KL divergence for one dimension simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}\\big(\\mathcal{N}(\\mu, \\sigma^2) \\parallel \\mathcal{N}(0,1)\\big) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - \\log \\sigma^2 - 1\\right).\n",
    "$$\n",
    "\n",
    "Summing over all dimensions $j = 1, \\dots, d$, we obtain the final result:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(q(z|x)\\parallel p(z)) = \\frac{1}{2}\\sum_{j=1}^{d} \\left( \\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1 \\right),\n",
    "$$\n",
    "\n",
    "which is equivalent to the form used in the code:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(q(z|x)\\parallel p(z)) = -\\frac{1}{2}\\sum_{j=1}^{d} \\left( 1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2 \\right).\n",
    "$$\n",
    "\n",
    "This closed-form formula for the KL divergence between Gaussian densities means that the KL term does not require any further numerical integration.\n",
    "\n",
    "Now your task is to implement this closed-form KL trick in the VAE loss function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41d627",
   "metadata": {},
   "source": [
    "[3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cab3a-5060-4f1a-af62-6d708be60430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vae_loss(model, x, beta):\n",
    "    \"\"\"\n",
    "    Perform a forward pass on and compute VAE loss.\n",
    "    \n",
    "    Args:\n",
    "        model: The VAE model.\n",
    "        x: A batch of training data.\n",
    "        beta: Weight for the KL divergence term.\n",
    "\n",
    "    **TODO:** Implement the closed-form KL divergence, referring to the\n",
    "    derivation above. Replace None with your code\n",
    "    \"\"\"\n",
    "    # Forward pass.\n",
    "    recon_x, mu, logvar = model(x)\n",
    "    \n",
    "    # Reconstruction loss (MSE).\n",
    "    recon_loss = torch.mean((recon_x - x) ** 2)\n",
    "\n",
    "    # ----- TODO: Student Implementation Starts Here -----\n",
    "    # KL Divergence loss:\n",
    "    kl_loss = None # TODO: fill in the closed-form KL loss based on the derivation above.\n",
    "    # ----- TODO: Student Implementation Ends Here -----\n",
    "\n",
    "    # Total loss.\n",
    "    loss = recon_loss + beta * kl_loss\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def train_vae(n_steps=1000, batch_size=128, lr=1e-3, beta=0.01):\n",
    "    \"\"\"\n",
    "    Train the VAE on Swiss Roll data.\n",
    "    \n",
    "    Args:\n",
    "        n_steps: Number of training steps.\n",
    "        batch_size: Number of samples per batch.\n",
    "        lr: Learning rate.\n",
    "        beta: Weight for the KL divergence term.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model and optimizer.\n",
    "    model = VAE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []  # To store loss values.\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training VAE\")\n",
    "    \n",
    "    for step in pbar:\n",
    "        # Get batch data.\n",
    "        x = generate_swiss_roll(batch_size).to(device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_vae_loss(model, x, beta)\n",
    "                \n",
    "        # Backpropagation.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    return model, losses, device\n",
    "\n",
    "# Train the VAE model.\n",
    "model, losses, device = train_vae()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9e37e-3dfe-48b8-8167-f27d03f3506c",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizing Reconstructions\n",
    "\n",
    "We now compare input points with their reconstructions. This helps us measure whether the VAE is learning an \n",
    "accurate latent representation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e12d4d-f2bd-4b1a-bac5-a6efee2e3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, n_points=1000):\n",
    "    \"\"\"\n",
    "    Visualize original points and their reconstructions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained VAE model.\n",
    "        n_points: Number of test points.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    x = generate_swiss_roll(n_points).to(device)\n",
    "    \n",
    "    # Get the reconstructions\n",
    "    with torch.no_grad():\n",
    "        recon_x, _, _ = model(x)\n",
    "    \n",
    "    # Convert tensors to numpy arrays.\n",
    "    x = x.cpu().numpy()\n",
    "    recon_x = recon_x.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # Plot original points.\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x[:, 0], x[:, 1], c='blue')\n",
    "    plt.title('Original Points')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Plot reconstructions.\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(recon_x[:, 0], recon_x[:, 1], c='red')\n",
    "    plt.title('Reconstructed Points')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Optionally, draw lines connecting corresponding points.\n",
    "    for i in range(n_points):\n",
    "        plt.plot([x[i, 0], recon_x[i, 0]], [x[i, 1], recon_x[i, 1]], 'k-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize reconstructions.\n",
    "visualize_reconstructions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce51368-c727-489e-bd3f-9ca67e420ab8",
   "metadata": {},
   "source": [
    "## Visualizing the Latent Space\n",
    "\n",
    "Here we generate a grid of latent vectors and decode them into data space. By comparing with real Swiss Roll data,\n",
    "you can assess how well the VAE has learned the latent structure.\n",
    "\n",
    "The grid is defined in the 2D latent space and then decoded to yield generated 2D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269645dd-3044-4d98-8cd0-073f6e62ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(model, n_points=1000):\n",
    "    \"\"\"\n",
    "    Visualize the learned latent space by decoding a grid of latent vectors.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained VAE model.\n",
    "        n_points: Number of grid points (should be a perfect square).\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Create a grid in latent space.\n",
    "    grid_size = int(np.sqrt(n_points))\n",
    "    x = np.linspace(-2, 2, grid_size)\n",
    "    y = np.linspace(-2, 2, grid_size)\n",
    "    z_grid = torch.tensor(np.array([[xi, yi] for xi in x for yi in y]), dtype=torch.float32).to(device)\n",
    "\n",
    "    # Plot real Swiss Roll data for reference.\n",
    "    real_data = generate_swiss_roll(n_points).numpy()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(real_data[:, 0], real_data[:, 1], c='red', alpha=0.5, label='Real')\n",
    "    \n",
    "    # Decode the latent grid.\n",
    "    with torch.no_grad():\n",
    "        decoded = model.decode(z_grid).cpu().numpy()\n",
    "    \n",
    "    # Plot the decoded points.\n",
    "    plt.scatter(decoded[:, 0], decoded[:, 1], c='blue', alpha=0.5, label='Generated')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('VAE Generated vs Real Data')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the latent space.\n",
    "visualize_latent_space(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d141dd",
   "metadata": {},
   "source": [
    "## Reflection & Analysis [7 points]\n",
    "\n",
    "Answer the following based on your implementation:\n",
    "\n",
    "**1. [2 points] The Reparameterization Trick [Understanding]**\n",
    "- Why do we need the reparameterization trick? What would happen if we sampled z directly from the distribution without this trick?\n",
    "\n",
    "> YOUR ANSWER:\n",
    "\n",
    "**2. [2 points] Training Dynamics [Observation]**\n",
    "- Visualize the latent space at steps 0, 500, and 1000. How does it evolve? What patterns emerge?\n",
    "\n",
    "> YOUR ANSWER:\n",
    "\n",
    "**3. [3 points] Beta Parameter [Experimentation]**\n",
    "- Try training with $\\beta$=0.001, $\\beta$=0.01, and $\\beta$=0.1. How does beta affect reconstruction quality vs latent space structure?\n",
    "\n",
    "> YOUR ANSWER:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdfca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39cef5a3-b407-4f1c-876e-9e0d7c9c6488",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Extra Material for Help and Reference \n",
    "## Understanding VAEs\n",
    "\n",
    "Imagine you're a curator at an art gallery, and you've been given an interesting challenge. You need to:\n",
    "1. Study thousands of paintings\n",
    "2. Learn to describe each painting using just a few numbers\n",
    "3. Be able to create new, similar paintings just from these number descriptions\n",
    "\n",
    "This is exactly what a VAE does! The \"encoder\" is like your brain learning to describe paintings, and the \"decoder\" is like your ability to imagine a painting from a description.\n",
    "\n",
    "### Why Is It \"Variational\"?\n",
    "\n",
    "Let's extend our analogy. When you describe a painting of a sunset, you might say \"lots of orange and red\" (let's say that's a number like 8.5 on a scale of 1-10). But should a painting with 8.4 or 8.6 orange-ness look completely different? Of course not!\n",
    "\n",
    "This is why VAEs don't just encode images to exact points, but to probability distributions. It's like saying \"this painting has orange-ness of about 8.5, give or take a bit\" rather than exactly 8.5. This \"give or take a bit\" is what makes it \"variational\"!\n",
    "\n",
    "## The Mathematics Breakdown\n",
    "\n",
    "### The Encoder: Turning Data into Distributions\n",
    "\n",
    "The encoder in a VAE doesn't just output a point in latent space. Instead, for each input x, it outputs:\n",
    "- μ (mu): The \"best guess\" for where this data point should be in latent space\n",
    "- σ (sigma): How much \"wiggle room\" we should allow around that guess\n",
    "\n",
    "Mathematically:\n",
    "```\n",
    "q_φ(z|x) = N(μ_φ(x), σ_φ(x))\n",
    "```\n",
    "\n",
    "Think of this like plotting a point on a map, but with a circle of uncertainty around it rather than just a dot.\n",
    "\n",
    "### The Decoder: Bringing Data Back\n",
    "\n",
    "The decoder takes points from this latent space and tries to reconstruct the original data. It's learning the reverse mapping:\n",
    "```\n",
    "p_θ(x|z)\n",
    "```\n",
    "\n",
    "### The Loss Function: A Balancing Act\n",
    "\n",
    "The VAE's loss function has two parts:\n",
    "1. Reconstruction Loss: \"How different is the output from the input?\"\n",
    "2. KL Divergence: \"How different is our latent distribution from a standard normal distribution?\"\n",
    "\n",
    "```\n",
    "L = E[log p_θ(x|z)] - D_KL(q_φ(z|x)||p(z))\n",
    "```\n",
    "\n",
    "This is like having two teachers grading your work:\n",
    "- One checking if your copy matches the original (reconstruction)\n",
    "- One checking if you're using your latent space efficiently (KL divergence)\n",
    "  \n",
    "## Further Reading\n",
    "\n",
    "For a deeper dive into VAEs, check out these papers:\n",
    "- Kingma & Welling (2014): \"Auto-Encoding Variational Bayes\"\n",
    "- Rezende et al. (2014): \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\"\n",
    "- Carl Doersch (2016): \"[Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
